{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from pytorch_lightning import Trainer, loggers\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchsummary import summary\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            # input (nc) x 128 x 128\n",
    "            nn.Conv2d(hparams.nc, hparams.nfe, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hparams.nfe),\n",
    "            nn.LeakyReLU(True),\n",
    "            # input (nfe) x 64 x 64\n",
    "            nn.Conv2d(hparams.nfe, hparams.nfe * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hparams.nfe * 2),\n",
    "            nn.LeakyReLU(True),\n",
    "            # input (nfe*2) x 32 x 32\n",
    "            nn.Conv2d(hparams.nfe * 2, hparams.nfe * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hparams.nfe * 4),\n",
    "            nn.LeakyReLU(True),\n",
    "            # input (nfe*4) x 16 x 16\n",
    "            nn.Conv2d(hparams.nfe * 4, hparams.nfe * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hparams.nfe * 8),\n",
    "            nn.LeakyReLU(True),\n",
    "            # input (nfe*8) x 8 x 8\n",
    "            nn.Conv2d(hparams.nfe * 8, hparams.nfe * 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hparams.nfe * 16),\n",
    "            nn.LeakyReLU(True),\n",
    "            # input (nfe*16) x 4 x 4\n",
    "            nn.Conv2d(hparams.nfe * 16, hparams.nz, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(hparams.nz),\n",
    "            nn.LeakyReLU(True)\n",
    "            # output (nz) x 1 x 1\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # input (nz) x 1 x 1\n",
    "            nn.ConvTranspose2d(hparams.nz, hparams.nfd * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(hparams.nfd * 16),\n",
    "            nn.ReLU(True),\n",
    "            # input (nfd*16) x 4 x 4\n",
    "            nn.ConvTranspose2d(hparams.nfd * 16, hparams.nfd * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hparams.nfd * 8),\n",
    "            nn.ReLU(True),\n",
    "            # input (nfd*8) x 8 x 8\n",
    "            nn.ConvTranspose2d(hparams.nfd * 8, hparams.nfd * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hparams.nfd * 4),\n",
    "            nn.ReLU(True),\n",
    "            # input (nfd*4) x 16 x 16\n",
    "            nn.ConvTranspose2d(hparams.nfd * 4, hparams.nfd * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hparams.nfd * 2),\n",
    "            nn.ReLU(True),\n",
    "            # input (nfd*2) x 32 x 32\n",
    "            nn.ConvTranspose2d(hparams.nfd * 2, hparams.nfd, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hparams.nfd),\n",
    "            nn.ReLU(True),\n",
    "            # input (nfd) x 64 x 64\n",
    "            nn.ConvTranspose2d(hparams.nfd, hparams.nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # output (nc) x 128 x 128\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=self.hparams.lr, betas=(self.hparams.beta1, self.hparams.beta2))\n",
    "\n",
    "    def save_images(self, x, output, name, n=16):\n",
    "        \"\"\"\n",
    "        Saves a plot of n images from input and output batch\n",
    "        \"\"\"\n",
    "\n",
    "        if self.hparams.batch_size < n:\n",
    "            raise IndexError(\"You are trying to plot more images than your batch contains!\")\n",
    "\n",
    "        # denormalize images\n",
    "        denormalization = transforms.Normalize((-MEAN / STD).tolist(), (1.0 / STD).tolist())\n",
    "        x = [denormalization(i) for i in x[:n]]\n",
    "        output = [denormalization(i) for i in output[:n]]\n",
    "\n",
    "        # make grids and save to logger\n",
    "        grid_top = vutils.make_grid(x, nrow=n)\n",
    "        grid_bottom = vutils.make_grid(output, nrow=n)\n",
    "        grid = torch.cat((grid_top, grid_bottom), 1)\n",
    "        self.logger.experiment.add_image(name, grid)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        output = self(x)\n",
    "        loss = F.mse_loss(output, x)\n",
    "\n",
    "        # save input and output images at beginning of epoch\n",
    "        if batch_idx == 0:\n",
    "            self.save_images(x, output, \"train_input_output\")\n",
    "\n",
    "        logs = {\"loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        output = self(x)\n",
    "        loss = F.mse_loss(output, x)\n",
    "        logs = {\"val_loss\": loss}\n",
    "        return {\"val_loss\": loss, \"log\": logs}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        logs = {\"avg_val_loss\": avg_loss}\n",
    "        return {\"avg_val_loss\": avg_loss, \"log\": logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        output = self(x)\n",
    "        loss = F.mse_loss(output, x)\n",
    "\n",
    "        # save input and output images at beginning of epoch\n",
    "        if batch_idx == 0:\n",
    "            self.save_images(x, output, \"test_input_output\")\n",
    "\n",
    "        logs = {\"test_loss\": loss}\n",
    "        return {\"test_loss\": loss, \"log\": logs}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        logs = {\"avg_test_loss\": avg_loss}\n",
    "        return {\"avg_test_loss\": avg_loss, \"log\": logs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str='./'):\n",
    "        super().__init()\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.hparams.image_size),\n",
    "                transforms.CenterCrop(self.hparams.image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(MEAN.tolist(), STD.tolist()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def prepare_data(self):\n",
    "        dataset = ImageFolder(root=self.hparams.data_root, transform=transform)\n",
    "\n",
    "        # train, val and test split taken from \"list_eval_partition.txt\" of original celebA paper\n",
    "        end_train_idx = 162770\n",
    "        end_val_idx = 182637\n",
    "        end_test_idx = len(dataset)\n",
    "\n",
    "        self.train_dataset = Subset(dataset, range(0, end_train_idx))\n",
    "        self.val_dataset = Subset(dataset, range(end_train_idx + 1, end_val_idx))\n",
    "        self.test_dataset = Subset(dataset, range(end_val_idx + 1, end_test_idx))\n",
    "\n",
    "    def setup(self):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.hparams.batch_size, shuffle=True, num_workers=self.hparams.num_workers\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 ('sklearn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3e861a57b07476ce47bbf487ddc13d988f12cc23750b4b0a980781898ced4f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
